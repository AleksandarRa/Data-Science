{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "986c1b8b-ad6a-4e6e-85e7-b03c221c2977",
   "metadata": {},
   "source": [
    "# Data Science | Lab: Natural Language Processing\n",
    "**Table of Contents:**  <a name=\"toc\"></a>\n",
    "1. [Accessing Websites](#crwaling)\n",
    "2. [Vectorizing Text](#vect)\n",
    "3. [Document Classification](#classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c033332-5bfa-493c-ab09-8cbebb395c7f",
   "metadata": {},
   "source": [
    "## Accessing Websites <a name=\"crawling\"></a>\n",
    "\n",
    "For this lab, we will rely on basic libararies for accessing websites: [requests](https://docs.python-requests.org/en/latest/) and [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) that support all steps involved in collecting website data:\n",
    "- **Crawler** Finding and downloading web pages automatically is called crawling. A program that\n",
    "downloads pages is called a web crawler.\n",
    "- **Parser** Interpreting and reconstructing the structure of a website is called parsing.\n",
    "- **Scraper** Extracting the content of a website (text only) is called scraping.\n",
    "\n",
    "In addition, [boilerpipe](https://pypi.org/project/boilerpy3/) can be used to automatically remove the boilerplate.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Politeness</b></br>\n",
    "    Be polite when crawling web pages! Do not fetch more than one page at a time from a particular web server. Wait in between requests. Respect the <tt>robots.txt</tt> file if provided.\n",
    "</div>\n",
    "\n",
    "[Back to top](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf27742-4cfd-4b5f-bbbc-061783ecbc94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install beautifulsoup4\n",
    "#!pip install requests\n",
    "\n",
    "import requests  # crawling\n",
    "from bs4 import BeautifulSoup  # parsing, scraping\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Ignoring deprecation warnings given by pandas when using the function `append()`\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6aca2ef-306b-4956-bc7d-9f166427324a",
   "metadata": {},
   "source": [
    "### Constructing the News Article Dataset\n",
    "The following code is used to read and store all links to articles categorized under the topics *artificial intelligence* and *neuroscience* at the [Wired UK](https://www.wired.co.uk) website. ``build_dataset`` takes a ``seed_url`` (topic overview page) as argument and stores the article headers and their links in a dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa5c6b4-37c7-4c01-9f4e-928856087a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(seed_url):\n",
    "    df = pd.DataFrame(columns=['topic', 'link', 'header'])\n",
    "    data = requests.get(seed_url)\n",
    "    soup = BeautifulSoup(data.content.decode('utf-8', 'ignore'), 'html.parser')\n",
    "    candidates = soup.find_all(\"a\", class_=[\"summary-item-tracking__hed-link\", \"summary-item__hed-link\"])\n",
    "    for article in candidates:\n",
    "        print(article.get_text().strip())\n",
    "        df = df.append({'topic': seed_url.split('/')[-1], 'link': article['href'], 'header': article.get_text().strip()}, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470639f7-994c-44c1-9240-11495a093d38",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Caution:</b> Call the following code only once (as rarely as possible) and than store and read the DataFrame locally.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b83dd4b-5d3b-43dd-a31d-758808edb226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawl and parse the AI page\n",
    "#seed_url = 'https://www.wired.co.uk/topic/artificial-intelligence'\n",
    "#df = build_dataset(seed_url)\n",
    "\n",
    "# Crawl and parse the neuroscience page\n",
    "#seed_url = 'https://www.wired.co.uk/topic/neuroscience'\n",
    "#df = df.append(build_dataset(seed_url), ignore_index=True)\n",
    "\n",
    "# Alternatively, read from file\n",
    "#df.to_csv('article_headers.csv', sep='\\t', index=False)\n",
    "df = pd.read_csv('article_headers.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570a0ca8-4c53-4acf-8296-6d3b91b3e4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.head())\n",
    "display(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1c5db5-d150-4861-862b-b6e28fc812de",
   "metadata": {},
   "source": [
    "[Back to top](#toc)\n",
    "\n",
    "## Vectorizing Text <a name=\"vect\"></a>\n",
    "Now, it's time to preprocess and vectorize the data and create the Bag-of-Words (BoW) vectors for each document. Before building a larger dataset, we will work with the **article headers as documents**.\n",
    "\n",
    "### The ``CountVectorizer``\n",
    "With the ``CountVectorizer``, scikit-learn offers an all-in-one solution for the preprocessing and vectorization of text data. Read through the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) and make yourself familiar with its usage. Make sure to understand\n",
    "* how tokenization is applied\n",
    "* how to remove low and high frequency words or stopwords\n",
    "* how to enable lowercasing\n",
    "* how to build a binary incidence matrix vs. a frequency matrix\n",
    "\n",
    "[Back to top](#toc)\n",
    "\n",
    "Vectorize the text by adding/changing the parameters when initializing the ``CountVectorizer`` in the following code cell. Make sure to\n",
    "* tokenize\n",
    "* lowercase\n",
    "* remove tokens that occur in less than two documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df16e8a0-2ea2-4026-8a95-f5fa6688c7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "train_data = df['header']\n",
    "\n",
    "vectorizer = CountVectorizer(lowercase = True, analyzer=\"word\", min_df=2, stop_words='english')\n",
    "X_train = vectorizer.fit_transform(train_data)\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e49b500-3f9b-4234-9794-50e9eaa39009",
   "metadata": {},
   "source": [
    "``vectorizer`` now holds the bag-of-words vector for each document. Analyze the object further by inspecting the following properties and methods:\n",
    "* ``vocabulary_`` \n",
    "* ``stop_words_``\n",
    "* ``get_feature_names``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de332296-f7e3-4bdc-afa9-f147e2082795",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectorizer.vocabulary_)\n",
    "print(vectorizer.stop_words_)  # all terms that are discraded by the vectorizer (e.g. defined as stopwords, frequency too low/high, ...)\n",
    "#print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b87e22-a9f2-4334-8007-2a6115e71adb",
   "metadata": {},
   "source": [
    "Afterwards, make sure to understand the difference between the following outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7255392d-0b4e-4914-a647-8ad0cd17920e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train)\n",
    "print(X_train.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d437cac-7585-45ae-9b3e-f8be229629d1",
   "metadata": {},
   "source": [
    "[Back to top](#toc)\n",
    "\n",
    "### The Document-Term Matrix\n",
    "Given this starter example of only article headers resulting in low-dimensional BoW vectors, it is possible to plot the resulting document-term-matrix as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ca7e1a-1ff7-47a8-bb7d-b2aded00600f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 5))\n",
    "sns.heatmap(X_train.todense(), ax=ax, cmap='gray_r')\n",
    "ax.set_xticks(range(X_train.shape[1]))\n",
    "ax.set_xticklabels(vectorizer.get_feature_names_out(), rotation=45)\n",
    "ax.set_xlabel('Dictionary')\n",
    "ax.set_ylabel(f'{X_train.shape[0]} Documents')\n",
    "ax.set_title('Document-Term-Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaea021-5669-44e8-b29d-2842fe06b2dc",
   "metadata": {},
   "source": [
    "You can also fit a ``TfidfVectorizer`` to see how the BoW vectors change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f45d12-ea08-4ae3-ba15-21718d6f8f94",
   "metadata": {},
   "source": [
    "## Document Classification <a name=\"classification\"></a>\n",
    "\n",
    "<img src=\"https://www.nltk.org/images/supervised-classification.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "[Back to top](#toc)\n",
    "\n",
    "Now, train a document classifier on the article texts to predict the article category (topic). \n",
    "\n",
    "- Use the [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) to extract the features of the article headers.\n",
    "- Use the MinDist classifier to predict the topics. Remember that in scikit-learn, the classifier is called [Nearest Centroid](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestCentroid.html?highlight=nearest%20centroid#sklearn.neighbors.NearestCentroid) classifier. Experiment with cosine distance instead of Euclidean distance.\n",
    "- In order to assess the classifier's performance, use the test dataset provided as ``test_articles.csv`` on Moodle and plot the confusion matrix."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af976cf5",
   "metadata": {},
   "source": [
    "### euclidean distance classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e3e041",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestCentroid\n",
    "#create model\n",
    "mindist = NearestCentroid()\n",
    "mindist.fit(X_train, df[\"topic\"])\n",
    "df_test = pd.read_csv('test_articles.csv', sep=\";\")\n",
    "x_test = vectorizer.transform(df_test[\"header\"])\n",
    "y_hat = mindist.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b9c012",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "print(mindist.score(x_test, df_test[\"topic\"]))\n",
    "\n",
    "cm = confusion_matrix(df_test[\"topic\"], y_hat, labels=mindist.classes_)\n",
    "ConfusionMatrixDisplay(cm, display_labels=mindist.classes_).plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "762df8e8",
   "metadata": {},
   "source": [
    "### cosine classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e033bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model\n",
    "mindist = NearestCentroid(metric=\"cosine\")\n",
    "mindist.fit(X_train, df[\"topic\"])\n",
    "df_test = pd.read_csv('test_articles.csv', sep=\";\")\n",
    "x_test = vectorizer.transform(df_test[\"header\"])\n",
    "y_hat = mindist.predict(x_test)\n",
    "print(mindist.score(x_test, df_test[\"topic\"]))\n",
    "\n",
    "cm = confusion_matrix(df_test[\"topic\"], y_hat, labels=mindist.classes_)\n",
    "ConfusionMatrixDisplay(cm, display_labels=mindist.classes_).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d488c41-e630-4a2d-9185-1e9462a81222",
   "metadata": {},
   "source": [
    "## Homework Assignment\n",
    "\n",
    "Extend your code to include the following:\n",
    "1. Add a third topic of your choice to the link list DataFrame. \n",
    "2. Access the website of each link in your list and extract the article text. Store it in a new columns in your DataFrame. (Make sure to be polite! Add ``time.sleep`` before accessing the next page.)\n",
    "3. Set up a grid search for at least three different parameters of the vectorizer.\n",
    "4. Evaluate the grid with 5-fold cross validation to find the best features for the MinDist classifier concerning F1 score when trained on the full article texts.\n",
    "5. Reuse the best model setting to evaluate the performance on the original test dataset (``test_articles.csv``).\n",
    "6. Plot the confusion matrix for the test dataset.\n",
    "\n",
    "## Moodle Upload\n",
    "Upload your notebook as ``firstname_lastname_nlp.html`` to Moodle. Make sure to consider the following:\n",
    "* Have all your import statements in one single cell at the top of the notebook.\n",
    "* Remove unnecessary code (such as plotting the document-term-matrix, experimenting with the properties and methods of the CountVectorizer, ...)\n",
    "* Print the head of your final DataFrame (containing article texts) once.\n",
    "* Include a markdown cell at the end where you:\n",
    "    * give a short overview of what your notebook is about\n",
    "    * describe and interpret your grid settings and justify your choices\n",
    "    * analyze the final/best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f476ce25-b93d-4c24-81b6-b93b2b0c1521",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
