{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e205d5d-216a-4782-abc8-bc975a4159d0",
   "metadata": {},
   "source": [
    "# Data Science | Lab: Image Processing\n",
    "**Table of Contents:**  <a name=\"toc\"></a>\n",
    "1. [Bag of Visual Words](#bovw)\n",
    "2. [Histogram of Visual Words](#hovw)\n",
    "3. [Image Classification](#classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604a8c29-fb83-4e34-b794-0ec6657850a3",
   "metadata": {},
   "source": [
    "# Bag of Visual Words\n",
    "Analogous to the Bag of Words technique which we covered in the last lab. This time we will extract \"visual\" words in order to classify pictures.\n",
    "<a name=\"bovw\"></a>\n",
    "<div style=\"width: 500px; text-align: center;\">\n",
    "    <img src=\"https://customers.pyimagesearch.com/wp-content/uploads/2015/09/bovw_image_example.jpg\"/>\n",
    "    <a href=\"https://customers.pyimagesearch.com/the-bag-of-visual-words-model/\" style=\"\">Source</a>\n",
    "</div>\n",
    "\n",
    "For the feature point extraction and for clustering we will rely on [OpenCV](https://docs.opencv.org/4.x/index.html) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e062036-102b-4490-b336-39c9f61546d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install opencv-contrib-python\n",
    "#%pip install imutils\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.cluster.vq import vq\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.metrics import accuracy_score\n",
    "import imutils\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42acfc34-0a64-406e-9e4a-df18dde42e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the image source folder\n",
    "path = \"Caltec_101\"\n",
    "# Choose three different classes individually\n",
    "use_classes= ['accordion', 'airplanes', 'chair']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3321ce2c-5f19-4fb5-8d3a-9f109968095d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This variable will store paths to each image\n",
    "X_paths = []\n",
    "# This variable will store class id as label\n",
    "y = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a289b97-333d-4ee1-9742-c44504737e88",
   "metadata": {},
   "source": [
    "### Constructing the dataset\n",
    "The following code uses the above defined ``path`` and ``use_classes`` to scan the given folders for pictures. Using all pictures stored in the given folders the dataset is going to be constructed. ``X_paths`` is storing paths to each image and ``y`` gives each picture a class id starting with 0. We are using images from [Caltech 101](https://docs.scipy.org/doc/scipy/tutorial/index.html#user-guide) dataset, which contains pictures of objects belonging to 101 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13430678-481e-4f4f-bbf0-9ce2e79660e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob \n",
    "\n",
    "for x,image in enumerate(use_classes):\n",
    "    old_amount = len(X_paths)\n",
    "    X_paths.extend(glob.glob(path+\"/\"+image+\"/*.jpg\"))\n",
    "    y.extend(np.array([x]*(len(X_paths)-old_amount)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f57e8b-bdee-4eea-a701-de5f53a27f30",
   "metadata": {},
   "source": [
    "### Train/Test split\n",
    "Perform a train/test split of the constructed dataset. Use 80% of data to train the model. Be sure to use stratified sampling since not all categories consist of equal number of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59feafc0-df27-4746-bac0-2e36f908be0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X_paths, y, train_size=0.8, random_state=42, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e56428-018b-4583-a5ad-8e1cc2f6a0d9",
   "metadata": {},
   "source": [
    "### Extract SIFT Features\n",
    "Use ``cv2`` ([OpenCV](https://docs.opencv.org/4.x/index.html)) to extract meaningful features which will be used as visual words further on. Loop through all given image paths and extract the descriptors of found keypoints using ``detectAndCompute()`` function described [here](https://docs.opencv.org/3.4/d0/d13/classcv_1_1Feature2D.html#a8be0d1c20b08eb867184b8d74c15a677). Moreover, useful info can be found in this [tutorial](https://docs.opencv.org/3.4/da/df5/tutorial_py_sift_intro.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472c49ff-ed12-4109-8702-798f0336c279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(x_train, sift):\n",
    "    desc_list = []\n",
    "    for path in x_train:\n",
    "        image = cv2.imread(path) \n",
    "        kp,desc = sift.detectAndCompute(image,None)\n",
    "        desc_list.append(desc)\n",
    "    \n",
    "    return desc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb61244-844b-44d4-a283-7bdee146dc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "sift = cv2.SIFT_create()\n",
    "train_desc_list = extract_features(x_train,sift)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09d7d76-7eb8-41e8-aebe-1cca6d1c58cd",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "Similar descriptors are building point clouds in hyperdimensional space. Be sure to use K-Means clustering method to extract clusters of descriptors from your descriptor list. Use sklearn's [KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) implementation and parameterize it carefully (i.e., minimize ``n_init`` and ``max_iter``). Otherwise, it will be really slow. OpenCV provides a faster implementation of [K-Means](https://docs.opencv.org/4.x/d1/d5c/tutorial_py_kmeans_opencv.html) and can be used as alternative. Use ``k=100`` as parameter for setting the number of classes\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Caution:</b> You will have to stack all the descriptors vertically in a numpy array in order to perform the clustering. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bb78e1-37d4-41cb-8a6e-d3f43659653d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack all the descriptors vertically in a numpy array\n",
    "descriptors = np.vstack(train_desc_list)\n",
    "print(descriptors[0].shape)\n",
    "print(descriptors[1].shape)\n",
    "print(len(descriptors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0e2cfe-5599-49a5-bcd9-113928fd2bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "k = 100\n",
    "kmeans = KMeans(n_clusters=k, random_state=0).fit(descriptors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cd1107-72da-4846-b640-f387c46ed512",
   "metadata": {},
   "source": [
    "# Histogram of Visual Words\n",
    "Now is the time to count the features of each picture. In this step we will create a histogram for every single picture. Suppose we chose 100 as the number of clusters for the K-Means clustering. After this step every picture should have a vector consisting of 100 elements. Each element in the vector represents one significant feature in the picture. \n",
    "<a name=\"hovw\"></a>\n",
    "<div style=\"width: 500px; text-align: center;\">\n",
    "    <img src=\"https://miro.medium.com/max/625/1*QgI1t-7yJApi4vQigFgsLQ.jpeg\"/>\n",
    "    <a href=\"https://towardsdatascience.com/bag-of-visual-words-in-a-nutshell-9ceea97ce0fb\" style=\"\">Source</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660fed20-2118-49d5-9392-2557f52e2026",
   "metadata": {},
   "source": [
    "### Constructing histogram\n",
    "Suppose that we computed 100 clusters from the descriptor list and we have 250 pictures in the training dataset. This function should create a vector for each picture containing 100 elements, hence the shape of the resulting variable will be ``(250, 100)``. Every picture contains an arbitary number of descriptors found in it, thus we need to loop over the pictures separately before computing the histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba6c9a4-9e31-408f-b20d-d91570886918",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_feature_histogram(_model, _desc_list, _k):\n",
    "    # Compute feature histogram\n",
    "    _bovw_features = np.zeros((len(_desc_list), _k),\"float32\")\n",
    "    for i, descr in enumerate(_desc_list):\n",
    "        labels = _model.predict(descr)\n",
    "        # compute histogram and store in _bovw_features\n",
    "        for l in labels:\n",
    "            _bovw_features[i][l]+=1\n",
    "    return _bovw_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8bec11-4914-4055-a220-e6e31e2eb043",
   "metadata": {},
   "outputs": [],
   "source": [
    "bovw_features_train = compute_feature_histogram(kmeans, train_desc_list, k) \n",
    "test_desc_list = extract_features(x_test, sift)\n",
    "bovw_features_test= compute_feature_histogram(kmeans, test_desc_list, k) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eab047b-6f78-40de-af7b-c02998fc5450",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bovw_features_train.shape)\n",
    "print(bovw_features_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a46e60-c138-4b5a-aeb9-23a5475bb6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb94ec7-ec34-404f-84ab-dbebe074cefe",
   "metadata": {},
   "source": [
    "# Image Classification\n",
    "Use the MinDist classifier to predict the category of each image. Remember that in scikit-learn, the classifier is called [Nearest Centroid](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestCentroid.html?highlight=nearest%20centroid#sklearn.neighbors.NearestCentroid) classifier.\n",
    "\n",
    "<a name=\"classification\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba18e7fb-eb09-49a0-8718-d6a2b61fec94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9a88f4-6154-4c4d-803a-a0c02c69240c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f75cc2e-d620-4f40-92cd-e4108652ccb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3eb9fe-26ee-43f8-954f-0b2526262de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_labels = [use_classes[i] for i in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15728eb0-cb07-4ae6-b86c-2f39a6fb09a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_path, prediction in zip(X_test_paths, prediction_labels):\n",
    "    image = cv2.imread(image_path)\n",
    "    cv2.namedWindow(\"Image\", cv2.WINDOW_NORMAL)\n",
    "    pt = (0, image.shape[0]-10)\n",
    "    cv2.putText(image, prediction, (0,10), cv2.FONT_HERSHEY_DUPLEX, 0.5, [0, 255, 0], 1)\n",
    "    cv2.imshow(\"Image\", image)\n",
    "    cv2.waitKey(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd93a9b-3e41-4048-9d36-4635f4f9d2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fffeb68-4db4-4168-922b-1ee102ea8af9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Homework Assignment\n",
    "\n",
    "Extend your code to include the following:\n",
    "1. Extend your dataset to use 5 different individually chosen categories of images.\n",
    "2. Set up a grid search for at least three different Ks for K-Means and minimum two different MinDist metrics. Evaluate the grid with 3-fold cross validation concerning the best accuracy.\n",
    "3. Try different scaling techniques for your dataset.\n",
    "4. Plot the confusion matrix for the test dataset using the best setting according to the grid search.\n",
    "5. Document your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c599ddd-678b-46ff-a101-771c2f337e5d",
   "metadata": {},
   "source": [
    "## Moodle Upload\n",
    "This is an **indivdual** assignment, meaning that you are graded individually. If you have collaborated with colleagues during the lab, make sure to state **all** of their names at the beginning of the document. The final document **must** exhibit individual efforts (structure, variable settings, reasoning, interpretation) despite some inherent similarities. \n",
    "\n",
    "Upload your notebook as ``firstname_lastname_ip.html`` to Moodle. \n",
    "\n",
    "Make sure to consider the following:\n",
    "* Have all your import statements in one single cell at the top of the notebook.\n",
    "* Remove unnecessary code.\n",
    "* Include a markdown cell at the end where you:\n",
    "    * give a short overview of what your notebook is about\n",
    "    * be sure to describe BoVW in your own words: Which steps are necessary? How does it relate to the BoW-concept from NLP? What are \"words\" and \"documents\" in this context?\n",
    "    * describe and interpret your settings and justify your choices\n",
    "    * analyze the final/best results\n",
    "\n",
    "**Deadline: 07.02.2023 23:59pm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51e7d46-1c7b-4425-ba25-950533644a93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
