{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Reuse the notebook from Lab 3 for the wine data. Make sure to\n",
    "####        * Reuse the same random seed throughout.\n",
    "####        * Use nearest neighbors\n",
    "#### 2) With using KFold to produce the data splits, implement cross validation. Make sure to store the predictions on each test fold and print the classification_report after having looped over all folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "x, y = load_wine(return_X_y = True)  #split into features X and labels y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=3, random_state=None, shuffle=True)\n",
    "\n",
    "result_array = []\n",
    "y_test_report = []\n",
    "y_predict_report = []\n",
    "\n",
    "for train_index, test_index in kf.split(x):\n",
    "    x_train, x_test = x[train_index],x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    y_test_report.extend(y_test)\n",
    "    scaler = StandardScaler(copy=True)\n",
    "    xTrain_scaled = scaler.fit_transform(x_train, y_train)\n",
    "    minDis = KNeighborsClassifier(n_neighbors=7)\n",
    "    minDis.fit(xTrain_scaled, y_train)\n",
    "    xTest_scaled = scaler.transform(x_test)\n",
    "    y_predict_report.extend(minDis.predict(xTest_scaled))\n",
    "    result_array.append(minDis.score(xTest_scaled, y_test))\n",
    "\n",
    "print('Average score: ', np.mean(result_array))\n",
    "\n",
    "## print the test reports\n",
    "print('The classification report:\\n')\n",
    "print(classification_report(y_test_report, y_predict_report))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Try with k=3‚Äã and k=10 folds.\n",
    "#### 4) In order to interpret the results (and fix possible issues), take a close look at the KFold visualization from the User Guide (not based on the wine data!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10, random_state=None, shuffle=True)\n",
    "\n",
    "result_array = []\n",
    "y_test_report = []\n",
    "y_predict_report = []\n",
    "\n",
    "for train_index, test_index in kf.split(x):\n",
    "    x_train, x_test = x[train_index],x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    y_test_report.extend(y_test)\n",
    "    scaler = StandardScaler(copy=True)\n",
    "    xTrain_scaled = scaler.fit_transform(x_train, y_train)\n",
    "    minDis = KNeighborsClassifier(n_neighbors=7)\n",
    "    minDis.fit(xTrain_scaled, y_train)\n",
    "    xTest_scaled = scaler.transform(x_test)\n",
    "    y_predict_report.extend(minDis.predict(xTest_scaled))\n",
    "    result_array.append(minDis.score(xTest_scaled, y_test))\n",
    "\n",
    "print('Average score: ', np.mean(result_array))\n",
    "## print the test reports\n",
    "print('The classification report:\\n')\n",
    "print(classification_report(y_test_report, y_predict_report))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Setting the shuffle parameter is very important since the classes are already ordered dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Implement Grid Search in combination with cross validation.\n",
    "####        * Use the following parameters from the KNeighborsClassifier for the grid: n_neighbors and p . Select reasonable values for both.\n",
    "####        * Implement a for loop to iterate over all combinations of the grid:\n",
    "#### 2) Run the Grid Search and print the classification report for each parameter combination.\n",
    "#### 3) Which parameter combination performs best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "n_neighbours = [2, 10]\n",
    "p = [1, 2]\n",
    "\n",
    "result_acb = {}\n",
    "\n",
    "for n_nei in n_neighbours:\n",
    "    for p_ in p:\n",
    "        kf = KFold(n_splits=10, random_state=None, shuffle=True)\n",
    "        result_array = []\n",
    "        result_acb[str(n_nei) + \" / \" + str(p_)] = {}\n",
    "        result_acb[str(n_nei) + \" / \" + str(p_)][\"Y_TEST\"] = []\n",
    "        result_acb[str(n_nei) + \" / \" + str(p_)][\"Y_PREDICT\"] = []\n",
    "        result_acb[str(n_nei) + \" / \" + str(p_)][\"Y_Score\"] = []\n",
    "\n",
    "        for train_index, test_index in kf.split(x):\n",
    "            x_train, x_test = x[train_index],x[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            result_acb[str(n_nei) + \" / \" + str(p_)][\"Y_TEST\"].extend(y_test)\n",
    "            scaler = StandardScaler(copy=True)\n",
    "            xTrain_scaled = scaler.fit_transform(x_train, y_train)\n",
    "            minDis = KNeighborsClassifier(n_neighbors=n_nei, p=p_ )\n",
    "            minDis.fit(xTrain_scaled, y_train)\n",
    "            xTest_scaled = scaler.transform(x_test)\n",
    "            result_acb[str(n_nei) + \" / \" + str(p_)][\"Y_PREDICT\"].extend(minDis.predict(xTest_scaled))\n",
    "            result_acb[str(n_nei) + \" / \" + str(p_)][\"Y_Score\"].append(minDis.score(xTest_scaled, y_test))\n",
    "\n",
    "\n",
    "## print the test reports\n",
    "for parameters_in in result_acb.keys():\n",
    "    print('Grid parameters:')\n",
    "    print(parameters_in)\n",
    "    print('Average score: ', np.mean(result_acb[parameters_in][\"Y_Score\"]))\n",
    "    print(classification_report(result_acb[parameters_in][\"Y_TEST\"], result_acb[parameters_in][\"Y_PREDICT\"]))\n",
    "    print('---------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### neighbour = 10 and manhattahn distance gets the best result "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Grid Search and Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Carefully read the documentation of üìù GridSearchCV, which combines the mechanisms of the grid search and the cross validation.\n",
    "#### 2) Reuse the kNeighborsClassifier and the ParameterGrid (check for correct naming).\n",
    "#### 3) Set the cross validation splitting strategy to k=10‚Äã folds.\n",
    "#### 4) Evaluate the results using GridSearchCV 's built-in methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {\"n_neighbors\":[2,10], \"p\":[1,2]}\n",
    "kn = KNeighborsClassifier()\n",
    "clf = GridSearchCV(kn, parameters, cv = 10)\n",
    "\n",
    "clf.fit(x,y)\n",
    "\n",
    "print(clf.best_estimator_.score)\n",
    "print(\"best score: \",clf.score(x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### As we see the result is the same as we got in the previous task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-6 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) Change the parameter scoring to use the F1 score for evaluation.\n",
    "#### 6) Find out how to store/access the best model parametrization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {\"n_neighbors\":[2,10], \"p\":[1,2]}\n",
    "kn = KNeighborsClassifier()\n",
    "# f1_weighted because we do not have only 0 and 1 as values\n",
    "clf = GridSearchCV(kn, parameters, cv = 10, scoring = \"f1_weighted\")\n",
    "\n",
    "clf.fit(x,y)\n",
    "estimator = clf.best_estimator_\n",
    "print(\"best value for n_neighbors parameter: \",estimator.get_params()['n_neighbors'])\n",
    "print(\"best value for p parameter: \",estimator.get_params()[\"p\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extend the grid with a parameter for switching the scaling of the data on/off. Then, for each test run made so far, enter the cross validation results in your table. Those values are more robust and reliable than those obtained from a single run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = load_wine(return_X_y = True)  #split into features X and labels y\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "n_neighbours = [2, 10]\n",
    "p = [1, 2]\n",
    "sc = [True, False]\n",
    "\n",
    "result_acb = {}\n",
    "\n",
    "for n_nei in n_neighbours:\n",
    "    for p_ in p:\n",
    "        for s in sc:\n",
    "            kf = KFold(n_splits=10, random_state=None, shuffle=True)\n",
    "            result_array = []\n",
    "            index_string = str(n_nei) + \" / \" + str(p_) + \" / \" + str(s)\n",
    "            result_acb[index_string] = {}\n",
    "            result_acb[index_string][\"Y_TEST\"] = []\n",
    "            result_acb[index_string][\"Y_PREDICT\"] = []\n",
    "            result_acb[index_string][\"Y_Score\"] = []\n",
    "\n",
    "            for train_index, test_index in kf.split(x):\n",
    "                x_train, x_test = x[train_index], x[test_index]\n",
    "                y_train, y_test = y[train_index], y[test_index]\n",
    "                if s:\n",
    "                    scaler = StandardScaler(copy=True)\n",
    "                    x_train = scaler.fit_transform(x_train, y_train)\n",
    "                    x_test = scaler.transform(x_test)\n",
    "\n",
    "                result_acb[index_string][\"Y_TEST\"].extend(y_test)\n",
    "                scaler = StandardScaler(copy=True)\n",
    "                xTrain_scaled = scaler.fit_transform(x_train, y_train)\n",
    "                minDis = KNeighborsClassifier(n_neighbors=n_nei, p=p_)\n",
    "                minDis.fit(xTrain_scaled, y_train)\n",
    "                xTest_scaled = scaler.transform(x_test)\n",
    "                result_acb[index_string][\"Y_PREDICT\"].extend(minDis.predict(xTest_scaled))\n",
    "                result_acb[index_string][\"Y_Score\"].append(minDis.score(xTest_scaled, y_test))\n",
    "\n",
    "## print the test reports\n",
    "for parameters_in in result_acb.keys():\n",
    "    print('Grid parameters:')\n",
    "    print(parameters_in)\n",
    "    print('Average score: ', np.mean(result_acb[parameters_in][\"Y_Score\"]))\n",
    "    print(classification_report(result_acb[parameters_in][\"Y_TEST\"], result_acb[parameters_in][\"Y_PREDICT\"]))\n",
    "    print('---------------------------------------------------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
