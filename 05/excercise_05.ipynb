{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many hidden layers are used?\n",
    "##### 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which activation function should be used in the hidden layer(s)?\n",
    "#### none linear e.g. ReLU, sigmuid  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which activation function should be used in the output layer?\n",
    "#### any or nothing if the softmay layer is used afterwards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which loss function is used?\n",
    "#### cross-entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Experimentation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a simple train-test split of the breast cancer data ( test_size=0.1 ) using random_state=42 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import numpy as np\n",
    "x,y = load_breast_cancer(return_X_y = True, as_frame = True)\n",
    "x.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement a Multi-layer Perceptron ( MLPClassifier ) with one hidden layer consisting of 64 hidden nodes with a ReLU activation and a softmax output layer to compute the cross entropy loss. Use a batch size of 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x ,y, train_size=0.9, random_state=42)\n",
    "\n",
    "scaler = StandardScaler(copy=True)\n",
    "xTrain_scaled = scaler.fit_transform(x_train, y_train)\n",
    "xTest_scaled = scaler.transform(x_test)\n",
    "\n",
    "clf = MLPClassifier(random_state=42, hidden_layer_sizes=(64),learning_rate_init=0.0001, batch_size=100, activation='relu')\n",
    "#clf.out_activation_ can not be overwritten, would change after fit again to logistic\n",
    "#clf.out_activation_='softmax'\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "\n",
    "print(clf.score(x_test,y_test))\n",
    "#print(clf.out_activation_)\n",
    "#print(clf.n_outputs_)\n",
    "print(classification_report(y_test, y_pred))\n",
    "plot_confusion_matrix(clf, x_test, y_test, cmap ='Blues')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the MLPClassifier with SGD optimizer and the default L2 regularization. No momentum (needs to be turned off).\n",
    "## Print/plot the classification report and the confusion matrix for the test data.\n",
    "## Plot the training loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def plot_learning_curve(df_loss_curve, title='Loss curve - Breast Cancer Data'):\n",
    "    sns.lineplot(data=df_loss_curve, dashes=False)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "max_iter = 200\n",
    "x_train, x_test, y_train, y_test = train_test_split(x ,y, train_size=0.9, random_state=42)\n",
    "\n",
    "scaler = StandardScaler(copy=True)\n",
    "xTrain_scaled = scaler.fit_transform(x_train, y_train)\n",
    "xTest_scaled = scaler.transform(x_test)\n",
    "\n",
    "clf = MLPClassifier(random_state=42, max_iter= max_iter, hidden_layer_sizes=[64],learning_rate_init=0.01, batch_size=100, activation='relu', solver = \"sgd\")\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "\n",
    "print(clf.score(x_test,y_test))\n",
    "print(classification_report(y_test, y_pred))\n",
    "#plot_confusion_matrix(clf, x_test, y_test, cmap ='Blues')\n",
    "\n",
    "loss_curve = clf.loss_curve_ + [np.NaN] * (max_iter - len(clf.loss_curve_))\n",
    "loss_curve_df = pd.DataFrame(loss_curve, columns=['SGD Baseline'])\n",
    "plot_learning_curve(loss_curve_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusing the same train-test split, experiment with different learning rates (by changing the value of learning_rate_init in the range between 0.0001 and 0.1 ) and interpret the changes in the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(df_loss_curve, learningRate, ax):\n",
    "    title = \"Loss curve with learning rate = \" + str(learningRate)\n",
    "    sns.lineplot(data=df_loss_curve, dashes=False, ax=ax)\n",
    "    ax.set_title(title)\n",
    "\n",
    "def myclassifer(learning_rate, xTrain, xTest, yTrain, yTest):\n",
    "    max_iter = 200\n",
    "\n",
    "    clf = MLPClassifier(random_state=42, max_iter= max_iter, hidden_layer_sizes=[64],learning_rate_init=learning_rate, batch_size=100, activation='relu', solver = \"sgd\")\n",
    "    clf.fit(xTrain, yTrain)\n",
    "    y_pred = clf.predict(xTest)\n",
    "\n",
    "    #print(clf.score(xTest, yTest))\n",
    "    #print(classification_report(yTest, y_pred))\n",
    "    return clf\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x ,y, train_size=0.9, random_state=42)\n",
    "scaler = StandardScaler(copy=True)\n",
    "xTrain_scaled = scaler.fit_transform(x_train, y_train)\n",
    "xTest_scaled = scaler.transform(x_test)\n",
    "\n",
    "learningrates =[0.0001, 0.001, 0.01, 0.1]\n",
    "\n",
    "fig, axs = plt.subplots(2,2,figsize=(15,20))\n",
    "\n",
    "i = 0\n",
    "for row in range (2):\n",
    "    for column in range(2):\n",
    "        clf = myclassifer(learningrates[i], x_train, x_test, y_train, y_test )\n",
    "        print(\"learning rate = \", learningrates[i])\n",
    "        xxx = clf.coefs_\n",
    "        print(\"weight of first layer = \", len(clf.coefs_[0]))\n",
    "        print(\"weight of second layer = \", len(clf.coefs_[1]))\n",
    "        loss_curve = clf.loss_curve_ + [np.NaN] * (max_iter - len(clf.loss_curve_))\n",
    "        loss_curve_df = pd.DataFrame(loss_curve, columns=['SGD Baseline'])\n",
    "        plot_learning_curve(loss_curve_df, learningrates[i], axs[row,column])\n",
    "        i+=1\n",
    "        print(\"----------------------------------------------------------\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the shape of the classifier's attribute coef_ (the weights of the network) and interpret it.\n",
    "#### we can see in the clf.coefs_[0] the weights of each input node and in clf.coeffs_[1] the weights of each hidden node"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Optimizers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next, experiment with different optimizers and settings. Use an initial learning rate of 0.0001 andalpha=0.001 throughout. Make sure to implement the following: \n",
    "#### * 'SGD Baseline' : SGD optimizer with default L2 regularization.\n",
    "#### * 'SGD with momentum' : SGD optimizer with default L2 regularization and a momentum of 0.9.\n",
    "#### * 'SGD with decreasing lr' : SGD optimizer with automatically decreasing learning rate.\n",
    "#### * 'Adam' : Adam optimizer with default L2 regularization and decay rates.\n",
    "\n",
    "#### Plot all training loss curves in a single plot and note the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x ,y, train_size=0.9, random_state=42)\n",
    "scaler = StandardScaler(copy=True)\n",
    "xTrain_scaled = scaler.fit_transform(x_train, y_train)\n",
    "xTest_scaled = scaler.transform(x_test)\n",
    "\n",
    "maxIter = 2000\n",
    "#alpha 0.0001 = L2 regularization\n",
    "clf = MLPClassifier(random_state=42, alpha=0.0001, max_iter= maxIter, hidden_layer_sizes=[64],\n",
    "    learning_rate_init=0.0001, batch_size=100, momentum=0, activation='relu', solver = \"sgd\")\n",
    "clf.fit(xTrain_scaled, y_train)\n",
    "loss_curve = clf.loss_curve_ + [np.NaN] * (max_iter - len(clf.loss_curve_))\n",
    "loss_curve_df = pd.DataFrame(loss_curve, columns=['SGD Baseline'])\n",
    "\n",
    "#SGD optimizer with default L2 regularization and a momentum of 0.9.\n",
    "clf = MLPClassifier(random_state=42, alpha=0.0001, max_iter= maxIter, hidden_layer_sizes=[64],\n",
    "    learning_rate_init=0.0001, batch_size=100, momentum=0.9, activation='relu', solver = \"sgd\")\n",
    "clf.fit(xTrain_scaled, y_train)\n",
    "loss_curve = clf.loss_curve_ + [np.NaN] * (max_iter - len(clf.loss_curve_))\n",
    "loss_curve_df['SGD with momentum'] = pd.DataFrame(loss_curve)\n",
    "\n",
    "#SGD optimizer with automatically decreasing learning rate.\n",
    "clf = MLPClassifier(random_state=42, alpha=0.0001, max_iter= maxIter, hidden_layer_sizes=[64],\n",
    "    learning_rate_init=0.0001, learning_rate = \"invscaling\", batch_size=100, momentum=0.9, activation='relu', solver = \"sgd\")\n",
    "clf.fit(xTrain_scaled, y_train)\n",
    "loss_curve = clf.loss_curve_ + [np.NaN] * (max_iter - len(clf.loss_curve_))\n",
    "loss_curve_df['SGD with decreasing lr'] = pd.DataFrame(loss_curve)\n",
    "\n",
    "#Adam optimizer with default L2 regularization and decay rates\n",
    "clf = MLPClassifier(random_state=42, alpha=0.0001, max_iter= maxIter, hidden_layer_sizes=[64],\n",
    "    learning_rate_init=0.0001, batch_size=100, activation='relu', solver = \"adam\")\n",
    "clf.fit(xTrain_scaled, y_train)\n",
    "loss_curve = clf.loss_curve_ + [np.NaN] * (max_iter - len(clf.loss_curve_))\n",
    "loss_curve_df['Adam'] = pd.DataFrame(loss_curve)\n",
    "\n",
    "title = \"Loss curves\"\n",
    "sns.lineplot(data=loss_curve_df, dashes=False)\n",
    "plt.title(title)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping Strategy\n",
    "#### Adapt the different classifiers/optimizer settings from above such that they stop early if the validation loss is increasing. This is used to prevent overfitting. Use 10% of the data for validation. Interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(copy=True)\n",
    "xTrain_scaled = scaler.fit_transform(x, y)\n",
    "\n",
    "maxIter = 2000\n",
    "#alpha 0.0001 = L2 regularization\n",
    "clf = MLPClassifier(random_state=42, alpha=0.0001, max_iter= maxIter, hidden_layer_sizes=[64],\n",
    "    learning_rate_init=0.0001, batch_size=100, momentum=0, activation='relu', solver = \"sgd\", early_stopping=True, n_iter_no_change=10)\n",
    "clf.fit(xTrain_scaled, y)\n",
    "loss_curve = clf.loss_curve_ + [np.NaN] * (maxIter - len(clf.loss_curve_))\n",
    "loss_curve_df = pd.DataFrame(loss_curve, columns=['SGD Baseline'])\n",
    "\n",
    "#SGD optimizer with default L2 regularization and a momentum of 0.9.\n",
    "clf = MLPClassifier(random_state=42, alpha=0.0001, max_iter= maxIter, hidden_layer_sizes=[64],\n",
    "    learning_rate_init=0.0001, batch_size=100, momentum=0.9, activation='relu', solver = \"sgd\", early_stopping=True, n_iter_no_change=10)\n",
    "clf.fit(xTrain_scaled, y)\n",
    "loss_curve = clf.loss_curve_ + [np.NaN] * (maxIter - len(clf.loss_curve_))\n",
    "loss_curve_df['SGD with momentum'] = pd.DataFrame(loss_curve)\n",
    "\n",
    "#SGD optimizer with automatically decreasing learning rate.\n",
    "clf = MLPClassifier(random_state=42, alpha=0.0001, max_iter= maxIter, hidden_layer_sizes=[64],\n",
    "    learning_rate_init=0.0001, learning_rate = \"invscaling\", batch_size=100, momentum=0.9, activation='relu', solver = \"sgd\", \n",
    "    early_stopping=True, n_iter_no_change=10)\n",
    "clf.fit(xTrain_scaled, y)\n",
    "loss_curve = clf.loss_curve_ + [np.NaN] * (maxIter - len(clf.loss_curve_))\n",
    "loss_curve_df['SGD with decreasing lr'] = pd.DataFrame(loss_curve)\n",
    "\n",
    "#Adam optimizer with default L2 regularization and decay rates\n",
    "clf = MLPClassifier(random_state=42, alpha=0.0001, max_iter= maxIter, hidden_layer_sizes=[64],\n",
    "    learning_rate_init=0.0001, batch_size=100, activation='relu', solver = \"adam\", early_stopping=True, n_iter_no_change=10)\n",
    "clf.fit(xTrain_scaled, y)\n",
    "loss_curve = clf.loss_curve_ + [np.NaN] * (maxIter - len(clf.loss_curve_))\n",
    "loss_curve_df['Adam'] = pd.DataFrame(loss_curve)\n",
    "\n",
    "title = \"Loss curves\"\n",
    "sns.lineplot(data=loss_curve_df, dashes=False)\n",
    "plt.title(title)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### this could mean that the classifier could overfit very fast"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "#### mplement a grid search with 5-fold cross validation using the GridSearchCV to find the best hyperparameter settings for the MLPClassifier . Make sure to include both the Adam optimizer and the SGD variant with momentum in the grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(copy=True)\n",
    "xTrain_scaled = scaler.fit_transform(x, y)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {  \"hidden_layer_sizes\":[[50,1],[64,1],[100,1], [50,2],[64,2],[100,2]], \n",
    "                \"learning_rate_init\":[0.0001, 0.001, 0.01],\n",
    "                \"activation\":['identity', 'logistic', 'tanh', 'relu'],\n",
    "                \"random_state\":[42],\"early_stopping\":[True],\n",
    "                \"alpha\":[0.0001, 0.001, 0.01],\n",
    "                \"solver\":[\"sgd\",\"adam\",\"lbfgs\"]}\n",
    "\n",
    "mlp = MLPClassifier()\n",
    "clf = GridSearchCV(mlp, parameters)\n",
    "clf.fit(xTrain_scaled,y)\n",
    "\n",
    "print(clf.best_estimator_.score)\n",
    "print(\"best score: \",clf.score(xTrain_scaled,y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
